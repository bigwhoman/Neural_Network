{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CE(y, v, epsilon=1e-15):\n",
    "    y = np.clip(y, epsilon, 1 - epsilon)\n",
    "    return -v*np.log(y)-(1-v)*np.log(1-y)\n",
    "\n",
    "\n",
    "def sRel(x):\n",
    "    return np.log(1+np.exp(np.clip(x, -709, 709)))\n",
    "\n",
    "def sig(z:float):\n",
    "    return np.clip(1/(1 + np.exp(-np.clip(z, -709, 709))), 1e-15, 1-1e-15)\n",
    "\n",
    "def sRelDer(x):\n",
    "    return np.exp(np.clip(x, -709, 709))/(1+np.exp(np.clip(x, -709, 709)))\n",
    "\n",
    "def CEDer(y, v):\n",
    "    epsilon = 1e-15\n",
    "    y = np.clip(y, epsilon, 1 - epsilon)\n",
    "    return (-v/y + (1-v)/(1-y))\n",
    "\n",
    "def sigDer(x):\n",
    "    return sig(x)*(1-sig(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    \n",
    "    \n",
    "    def __init__(self,  loss_func,loss_der, backward_func, forward_funcs, layers=2, neurons_per_layer=2, initial_x=2):\n",
    "        self.W = []\n",
    "        self.layers = layers\n",
    "        self.W.append(np.random.randn(neurons_per_layer,initial_x+1))\n",
    "        #determin the hidden layers\n",
    "        for layer in range(layers-2):\n",
    "            self.W.append(np.random.randn(neurons_per_layer, neurons_per_layer+1))\n",
    "        # Hypothetically say we only have one output from this NN\n",
    "        self.W.append(np.random.randn(1, neurons_per_layer+1))\n",
    "        self.loss_func = loss_func\n",
    "        self.forward_funcs = forward_funcs\n",
    "        self.loss_der = loss_der\n",
    "        self.backward_func = backward_func\n",
    "\n",
    "\n",
    "    # -------------------------- forward prop\n",
    "    def forward_prop(self,X, v, W):\n",
    "        y = []\n",
    "        z = []\n",
    "\n",
    "        y0 = X\n",
    "\n",
    "        y0 = np.insert(y0, 0, 1)\n",
    "\n",
    "        y.append(y0)\n",
    "\n",
    "        # print(y0)\n",
    "\n",
    "\n",
    "        for i in range(len(W)):\n",
    "            # print(W[i].shape, y[-1].shape)\n",
    "            zi = W[i] @ y[-1]\n",
    "\n",
    "            z.append(zi)\n",
    "\n",
    "            # print(f\"z{i+1} ---> {zi}\")\n",
    "\n",
    "            yoo = self.forward_funcs[i](zi)\n",
    "            if(i < len(W) - 1):\n",
    "                yoo = np.insert(yoo, 0, 1)\n",
    "\n",
    "            y.append(yoo)\n",
    "\n",
    "            # print(f\"y{i+1} ---> {yoo}\")\n",
    "\n",
    "\n",
    "        Loss = self.loss_func(y[-1], v)\n",
    "        # print(\"Loss --->\",Loss)\n",
    "\n",
    "        return (y,z,Loss)\n",
    "\n",
    "\n",
    "    # --------------------------------------- Backward Propagation\n",
    "    def backward_prob(self, W, y, z, v):\n",
    "        yp = []\n",
    "        zp = []\n",
    "        dW = []\n",
    "\n",
    "        y2p = self.loss_der(y[len(W)], v)\n",
    "        z2p = self.backward_func[self.layers-1](z[len(W)-1]) * y2p\n",
    "\n",
    "        yp.append(y2p)\n",
    "        zp.append(z2p)\n",
    "        \n",
    "\n",
    "        # -------------------------\n",
    "        # print(f\"y2p ---> {y2p} ------ z2p ---> {z2p}\")\n",
    "        for i in range(len(W)-1):\n",
    "            yop = W[len(W)-1-i].T @ zp[-1] \n",
    "            yop = yop[1:]\n",
    "            zop = self.backward_func[self.layers - 1 - i](z[len(W)-i-2]) * yop\n",
    "\n",
    "            yp.append(yop)\n",
    "            zp.append(zop)\n",
    "\n",
    "            # print(f\"y1p --> {yop} ---- z1p ---> {zop}\")\n",
    "\n",
    "\n",
    "        yp.reverse()\n",
    "        zp.reverse()\n",
    "        for i in range(len(W)):\n",
    "            zpi = zp[i].reshape(-1,1)\n",
    "            y[i] = y[i].reshape(-1,1)\n",
    "\n",
    "        \n",
    "            doW = zpi @ y[i].T \n",
    "\n",
    "            dW.append(doW)\n",
    "\n",
    "        return dW\n",
    "        \n",
    "\n",
    "    def train(self, dataset, lr=0.01):\n",
    "        delta = 10000000000000000000\n",
    "        Loss = 0\n",
    "        Old_Loss = 1000000\n",
    "        total_loss = 0\n",
    "        while delta > 0.0000001 : \n",
    "            \n",
    "            Old_Loss = total_loss\n",
    "            total_loss = 0\n",
    "            dWs = []\n",
    "            for data in dataset :\n",
    "                y,z,Loss = self.forward_prop(data[0], data[1], self.W)\n",
    "                total_loss += Loss\n",
    "                dW = self.backward_prob(self.W, y, z, data[1])\n",
    "                \n",
    "                dWs.append(dW)\n",
    "            \n",
    "            avg_dW = []\n",
    "            for layer in range(len(self.W)):\n",
    "                # Get all gradients for this layer\n",
    "                layer_gradients = [dWs[i][layer] for i in range(len(dWs))]\n",
    "                # Calculate mean gradient for this layer\n",
    "                layer_mean = np.mean(layer_gradients, axis=0)\n",
    "                avg_dW.append(layer_mean)\n",
    "\n",
    "            for i in range(len(self.W)):\n",
    "                self.W[i] -= lr * avg_dW[i]\n",
    "            delta = abs(Old_Loss - total_loss)\n",
    "        \n",
    "        \n",
    "    def test(self, dataset):\n",
    "        for data in dataset :\n",
    "            y,z,Loss = self.forward_prop(data[0], data[1], self.W)\n",
    "            print(data[0],y[-1], Loss)\n",
    "\n",
    "\n",
    "dataset = [[[1,1],0], [[0,0],0], [[1,0],1], [[0,1],1]]\n",
    "\n",
    "layers = 20\n",
    "forward_funcs = [sRel for i in range(layers - 1)]\n",
    "forward_funcs.append(sig)\n",
    "\n",
    "backward_funcs = [sRelDer for i in range(layers - 1)]\n",
    "backward_funcs.append(sigDer)\n",
    "\n",
    "nur = NN(layers=layers, initial_x=len(dataset[0][0]), \n",
    "         loss_func=CE,loss_der=CEDer, forward_funcs=forward_funcs, neurons_per_layer=10, backward_func=backward_funcs)\n",
    "\n",
    "nur.train(dataset, lr=0.1)\n",
    "\n",
    "nur.test(dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1. 1. 1.]\n",
    "z1 ---> [0.3 0.3]\n",
    "y1 ---> [0.85435524 0.85435524]\n",
    "z2 ---> [0.27087105]\n",
    "y2 ---> [0.56730673]\n",
    "Loss ---> [0.83772619]\n",
    "y2p ---> [-2.31110599] ------ z2p ---> [-0.56730673]\n",
    "y1p --> [-0.05673067 -0.05673067] ---- z1p ---> [-0.03258851 -0.03258851]\n",
    "--------------dW----------------\n",
    " [array([[-0.03258851, -0.03258851, -0.03258851],\n",
    "       [-0.03258851, -0.03258851, -0.03258851]]), array([[-0.56730673, -0.48468148, -0.48468148]])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
